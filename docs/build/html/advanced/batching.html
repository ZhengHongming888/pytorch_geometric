<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Advanced Mini-Batching &mdash; pytorch_geometric  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/mytheme.css" type="text/css" />
    <link rel="shortcut icon" href="https://raw.githubusercontent.com/pyg-team/pyg_sphinx_theme/master/pyg_sphinx_theme/static/img/favicon.png"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/on_pyg_load.js"></script>
        <script src="../_static/js/version_alert.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Memory-Efficient Aggregations" href="sparse_tensor.html" />
    <link rel="prev" title="Multi-Node Training using SLURM" href="../tutorial/multi_node_multi_gpu_vanilla.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html">
            
              <img src="https://raw.githubusercontent.com/pyg-team/pyg_sphinx_theme/master/pyg_sphinx_theme/static/img/pyg_logo.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                2.4.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Install PyG</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/installation.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started/introduction.html">Introduction by Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/colabs.html">Colab Notebooks and Video Tutorials</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorial/gnn_design.html">Design of Graph Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial/dataset.html">Working with Graph Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial/application.html">Use-Cases &amp; Applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial/multi_gpu.html">Multi-GPU Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Concepts</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Advanced Mini-Batching</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#pairs-of-graphs">Pairs of Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bipartite-graphs">Bipartite Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#batching-along-new-dimensions">Batching Along New Dimensions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="sparse_tensor.html">Memory-Efficient Aggregations</a></li>
<li class="toctree-l1"><a class="reference internal" href="hgam.html">Hierarchical Neighborhood Sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile.html">Compiled Graph Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">TorchScript Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="remote.html">Scaling Up GNNs via Remote Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="graphgym.html">Managing Experiments with GraphGym</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpu_affinity.html">CPU Affinity for PyG Workloads</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../modules/root.html">torch_geometric</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/nn.html">torch_geometric.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/data.html">torch_geometric.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/loader.html">torch_geometric.loader</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/sampler.html">torch_geometric.sampler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/datasets.html">torch_geometric.datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/transforms.html">torch_geometric.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/utils.html">torch_geometric.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/explain.html">torch_geometric.explain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/contrib.html">torch_geometric.contrib</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/graphgym.html">torch_geometric.graphgym</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/profile.html">torch_geometric.profile</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cheatsheets</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../cheatsheet/gnn_cheatsheet.html">GNN Cheatsheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cheatsheet/data_cheatsheet.html">Dataset Cheatsheet</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">External Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../external/resources.html">External Resources</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">pytorch_geometric</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Advanced Mini-Batching</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/advanced/batching.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="advanced-mini-batching">
<h1>Advanced Mini-Batching<a class="headerlink" href="#advanced-mini-batching" title="Permalink to this heading"></a></h1>
<p>The creation of mini-batching is crucial for letting the training of a deep learning model scale to huge amounts of data.
Instead of processing examples one-by-one, a mini-batch groups a set of examples into a unified representation where it can efficiently be processed in parallel.
In the image or language domain, this procedure is typically achieved by rescaling or padding each example into a set to equally-sized shapes, and examples are then grouped in an additional dimension.
The length of this dimension is then equal to the number of examples grouped in a mini-batch and is typically referred to as the <code class="xref py py-obj docutils literal notranslate"><span class="pre">batch_size</span></code>.</p>
<p>Since graphs are one of the most general data structures that can hold <em>any</em> number of nodes or edges, the two approaches described above are either not feasible or may result in a lot of unnecessary memory consumption.
In <span class="inline-logo pyg">PyG</span>, we opt for another approach to achieve parallelization across a number of examples.
Here, adjacency matrices are stacked in a diagonal fashion (creating a giant graph that holds multiple isolated subgraphs), and node and target features are simply concatenated in the node dimension, <em>i.e.</em></p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{A} = \begin{bmatrix} \mathbf{A}_1 &amp; &amp; \\ &amp; \ddots &amp; \\ &amp; &amp; \mathbf{A}_n \end{bmatrix}, \qquad \mathbf{X} = \begin{bmatrix} \mathbf{X}_1 \\ \vdots \\ \mathbf{X}_n \end{bmatrix}, \qquad \mathbf{Y} = \begin{bmatrix} \mathbf{Y}_1 \\ \vdots \\ \mathbf{Y}_n \end{bmatrix}.\end{split}\]</div>
<p>This procedure has some crucial advantages over other batching procedures:</p>
<ol class="arabic simple">
<li><p>GNN operators that rely on a message passing scheme do not need to be modified since messages still cannot be exchanged between two nodes that belong to different graphs.</p></li>
<li><p>There is no computational or memory overhead.
For example, this batching procedure works completely without any padding of node or edge features.
Note that there is no additional memory overhead for adjacency matrices since they are saved in a sparse fashion holding only non-zero entries, <em>i.e.</em>, the edges.</p></li>
</ol>
<p><span class="inline-logo pyg">PyG</span> automatically takes care of batching multiple graphs into a single giant graph with the help of the <a class="reference internal" href="../modules/loader.html#torch_geometric.loader.DataLoader" title="torch_geometric.loader.DataLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch_geometric.loader.DataLoader</span></code></a> class.
Internally, <a class="reference internal" href="../modules/loader.html#torch_geometric.loader.DataLoader" title="torch_geometric.loader.DataLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code></a> is just a regular <span class="inline-logo pytorch">PyTorch</span> <a class="reference external" href="https://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code></a> that overwrites its <code class="xref py py-func docutils literal notranslate"><span class="pre">collate()</span></code> functionality, <em>i.e.</em>, the definition of how a list of examples should be grouped together.
Therefore, all arguments that can be passed to a <span class="inline-logo pytorch">PyTorch</span> <a class="reference external" href="https://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader" title="(in PyTorch vmaster (2.1.0a0+gitbe0b12e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code></a> can also be passed to a <span class="inline-logo pyg">PyG</span> <a class="reference internal" href="../modules/loader.html#torch_geometric.loader.DataLoader" title="torch_geometric.loader.DataLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code></a>, <em>e.g.</em>, the number of workers <code class="xref py py-obj docutils literal notranslate"><span class="pre">num_workers</span></code>.</p>
<p>In its most general form, the <span class="inline-logo pyg">PyG</span> <a class="reference internal" href="../modules/loader.html#torch_geometric.loader.DataLoader" title="torch_geometric.loader.DataLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code></a> will automatically increment the <code class="xref py py-obj docutils literal notranslate"><span class="pre">edge_index</span></code> tensor by the cumulated number of nodes of all graphs that got collated before the currently processed graph, and will concatenate <code class="xref py py-obj docutils literal notranslate"><span class="pre">edge_index</span></code> tensors (that are of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">[2,</span> <span class="pre">num_edges]</span></code>) in the second dimension.
The same is true for <code class="xref py py-obj docutils literal notranslate"><span class="pre">face</span></code> tensors, <em>i.e.</em>, face indices in meshes.
All other tensors will just get concatenated in the first dimension without any further increasement of their values.</p>
<p>However, there are a few special use-cases (as outlined below) where the user actively wants to modify this behavior to its own needs.
<span class="inline-logo pyg">PyG</span> allows modification to the underlying batching procedure by overwriting the <a class="reference internal" href="../generated/torch_geometric.data.Data.html#torch_geometric.data.Data.__inc__" title="torch_geometric.data.Data.__inc__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch_geometric.data.Data.__inc__()</span></code></a> and <a class="reference internal" href="../generated/torch_geometric.data.Data.html#torch_geometric.data.Data.__cat_dim__" title="torch_geometric.data.Data.__cat_dim__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch_geometric.data.Data.__cat_dim__()</span></code></a> functionalities.
Without any modifications, these are defined as follows in the <a class="reference internal" href="../generated/torch_geometric.data.Data.html#torch_geometric.data.Data" title="torch_geometric.data.Data"><code class="xref py py-class docutils literal notranslate"><span class="pre">Data</span></code></a> class:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">__inc__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="s1">&#39;index&#39;</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_nodes</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">__cat_dim__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="s1">&#39;index&#39;</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
</pre></div>
</div>
<p>We can see that <a class="reference internal" href="../generated/torch_geometric.data.Data.html#torch_geometric.data.Data.__inc__" title="torch_geometric.data.Data.__inc__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">__inc__()</span></code></a> defines the incremental count between two consecutive graph attributes.
By default, <span class="inline-logo pyg">PyG</span> increments attributes by the number of nodes whenever their attribute names contain the substring <code class="xref py py-obj docutils literal notranslate"><span class="pre">index</span></code> (for historical reasons), which comes in handy for attributes such as <code class="xref py py-obj docutils literal notranslate"><span class="pre">edge_index</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">node_index</span></code>.
However, note that this may lead to unexpected behavior for attributes whose names contain the substring <code class="xref py py-obj docutils literal notranslate"><span class="pre">index</span></code> but should not be incremented.
To make sure, it is best practice to always double-check the output of batching.
Furthermore, <a class="reference internal" href="../generated/torch_geometric.data.Data.html#torch_geometric.data.Data.__cat_dim__" title="torch_geometric.data.Data.__cat_dim__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">__cat_dim__()</span></code></a> defines in which dimension graph tensors of the same attribute should be concatenated together.
Both functions are called for each attribute stored in the <a class="reference internal" href="../generated/torch_geometric.data.Data.html#torch_geometric.data.Data" title="torch_geometric.data.Data"><code class="xref py py-class docutils literal notranslate"><span class="pre">Data</span></code></a> class, and get passed their specific <code class="xref py py-obj docutils literal notranslate"><span class="pre">key</span></code> and value <code class="xref py py-obj docutils literal notranslate"><span class="pre">item</span></code> as arguments.</p>
<p>In what follows, we present a few use-cases where the modification of <a class="reference internal" href="../generated/torch_geometric.data.Data.html#torch_geometric.data.Data.__inc__" title="torch_geometric.data.Data.__inc__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">__inc__()</span></code></a> and <a class="reference internal" href="../generated/torch_geometric.data.Data.html#torch_geometric.data.Data.__cat_dim__" title="torch_geometric.data.Data.__cat_dim__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">__cat_dim__()</span></code></a> might be absolutely necessary.</p>
<section id="pairs-of-graphs">
<h2>Pairs of Graphs<a class="headerlink" href="#pairs-of-graphs" title="Permalink to this heading"></a></h2>
<p>In case you want to store multiple graphs in a single <a class="reference internal" href="../generated/torch_geometric.data.Data.html#torch_geometric.data.Data" title="torch_geometric.data.Data"><code class="xref py py-class docutils literal notranslate"><span class="pre">Data</span></code></a> object, <em>e.g.</em>, for applications such as graph matching, you need to ensure correct batching behavior across all those graphs.
For example, consider storing two graphs, a source graph <span class="math notranslate nohighlight">\(\mathcal{G}_s\)</span> and a target graph <span class="math notranslate nohighlight">\(\mathcal{G}_t\)</span> in a <a class="reference internal" href="../generated/torch_geometric.data.Data.html#torch_geometric.data.Data" title="torch_geometric.data.Data"><code class="xref py py-class docutils literal notranslate"><span class="pre">Data</span></code></a>, <em>e.g.</em>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch_geometric.data</span> <span class="kn">import</span> <span class="n">Data</span>

<span class="k">class</span> <span class="nc">PairData</span><span class="p">(</span><span class="n">Data</span><span class="p">):</span>
    <span class="k">pass</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">PairData</span><span class="p">(</span><span class="n">x_s</span><span class="o">=</span><span class="n">x_s</span><span class="p">,</span> <span class="n">edge_index_s</span><span class="o">=</span><span class="n">edge_index_s</span><span class="p">,</span>  <span class="c1"># Source graph.</span>
                <span class="n">x_t</span><span class="o">=</span><span class="n">x_t</span><span class="p">,</span> <span class="n">edge_index_t</span><span class="o">=</span><span class="n">edge_index_t</span><span class="p">)</span>  <span class="c1"># Target graph.</span>
</pre></div>
</div>
<p>In this case, <code class="xref py py-obj docutils literal notranslate"><span class="pre">edge_index_s</span></code> should be increased by the number of nodes in the source graph <span class="math notranslate nohighlight">\(\mathcal{G}_s\)</span>, <em>e.g.</em>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">x_s.size(0)</span></code>, and <code class="xref py py-obj docutils literal notranslate"><span class="pre">edge_index_t</span></code> should be increased by the number of nodes in the target graph <span class="math notranslate nohighlight">\(\mathcal{G}_t\)</span>, <em>e.g.</em>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">x_t.size(0)</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PairData</span><span class="p">(</span><span class="n">Data</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__inc__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s1">&#39;edge_index_s&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_s</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s1">&#39;edge_index_t&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_t</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__inc__</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<p>We can test our <code class="xref py py-class docutils literal notranslate"><span class="pre">PairData</span></code> batching behavior by setting up a simple test script:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch_geometric.loader</span> <span class="kn">import</span> <span class="n">DataLoader</span>

 <span class="n">x_s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>  <span class="c1"># 5 nodes.</span>
 <span class="n">edge_index_s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
 <span class="p">])</span>

 <span class="n">x_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>  <span class="c1"># 4 nodes.</span>
 <span class="n">edge_index_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
 <span class="p">])</span>

 <span class="n">data</span> <span class="o">=</span> <span class="n">PairData</span><span class="p">(</span><span class="n">x_s</span><span class="o">=</span><span class="n">x_s</span><span class="p">,</span> <span class="n">edge_index_s</span><span class="o">=</span><span class="n">edge_index_s</span><span class="p">,</span>
                 <span class="n">x_t</span><span class="o">=</span><span class="n">x_t</span><span class="p">,</span> <span class="n">edge_index_t</span><span class="o">=</span><span class="n">edge_index_t</span><span class="p">)</span>

 <span class="n">data_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="p">]</span>
 <span class="n">loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">data_list</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
 <span class="n">batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">loader</span><span class="p">))</span>

 <span class="nb">print</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
 <span class="o">&gt;&gt;&gt;</span> <span class="n">PairDataBatch</span><span class="p">(</span><span class="n">x_s</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span> <span class="n">edge_index_s</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
                   <span class="n">x_t</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span> <span class="n">edge_index_t</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>

 <span class="nb">print</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">edge_index_s</span><span class="p">)</span>
 <span class="o">&gt;&gt;&gt;</span> <span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
             <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>

 <span class="nb">print</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">edge_index_t</span><span class="p">)</span>
 <span class="o">&gt;&gt;&gt;</span> <span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
             <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]])</span>
</pre></div>
</div>
<p>Everything looks good so far!
<code class="xref py py-obj docutils literal notranslate"><span class="pre">edge_index_s</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">edge_index_t</span></code> get correctly batched together, even when using a different numbers of nodes for <span class="math notranslate nohighlight">\(\mathcal{G}_s\)</span> and <span class="math notranslate nohighlight">\(\mathcal{G}_t\)</span>.
However, the <code class="xref py py-obj docutils literal notranslate"><span class="pre">batch</span></code> attribute (that maps each node to its respective graph) is missing since <span class="inline-logo pyg">PyG</span> fails to identify the actual graph in the <code class="xref py py-class docutils literal notranslate"><span class="pre">PairData</span></code> object.
That is where the <code class="xref py py-obj docutils literal notranslate"><span class="pre">follow_batch</span></code> argument of the <a class="reference internal" href="../modules/loader.html#torch_geometric.loader.DataLoader" title="torch_geometric.loader.DataLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code></a> comes into play.
Here, we can specify for which attributes we want to maintain the batch information:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">data_list</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">follow_batch</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x_s&#39;</span><span class="p">,</span> <span class="s1">&#39;x_t&#39;</span><span class="p">])</span>
<span class="n">batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">loader</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">PairDataBatch</span><span class="p">(</span><span class="n">x_s</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span> <span class="n">edge_index_s</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="n">x_s_batch</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">],</span>
                  <span class="n">x_t</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span> <span class="n">edge_index_t</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="n">x_t_batch</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">x_s_batch</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">x_t_batch</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<p>As one can see, <code class="xref py py-obj docutils literal notranslate"><span class="pre">follow_batch=['x_s',</span> <span class="pre">'x_t']</span></code> now successfully creates assignment vectors <code class="xref py py-obj docutils literal notranslate"><span class="pre">x_s_batch</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">x_t_batch</span></code> for the node features <code class="xref py py-obj docutils literal notranslate"><span class="pre">x_s</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">x_t</span></code>, respectively.
That information can now be used to perform reduce operations, <em>e.g.</em>, global pooling, on multiple graphs in a single <code class="xref py py-class docutils literal notranslate"><span class="pre">Batch</span></code> object.</p>
</section>
<section id="bipartite-graphs">
<h2>Bipartite Graphs<a class="headerlink" href="#bipartite-graphs" title="Permalink to this heading"></a></h2>
<p>The adjacency matrix of a bipartite graph defines the relationship between nodes of two different node types.
In general, the number of nodes for each node type do not need to match, resulting in a non-quadratic adjacency matrix of shape <span class="math notranslate nohighlight">\(\mathbf{A} \in \{ 0, 1 \}^{N \times M}\)</span> with <span class="math notranslate nohighlight">\(N \neq M\)</span> potentially.
In a mini-batching procedure of bipartite graphs, the source nodes of edges in <code class="xref py py-obj docutils literal notranslate"><span class="pre">edge_index</span></code> should get increased differently than the target nodes of edges in <code class="xref py py-obj docutils literal notranslate"><span class="pre">edge_index</span></code>.
To achieve this, consider a bipartite graph between two node types with corresponding node features <code class="xref py py-obj docutils literal notranslate"><span class="pre">x_s</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">x_t</span></code>, respectively:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch_geometric.data</span> <span class="kn">import</span> <span class="n">Data</span>

<span class="k">class</span> <span class="nc">BipartiteData</span><span class="p">(</span><span class="n">Data</span><span class="p">):</span>
    <span class="k">pass</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">BipartiteData</span><span class="p">(</span><span class="n">x_s</span><span class="o">=</span><span class="n">x_s</span><span class="p">,</span> <span class="n">x_t</span><span class="o">=</span><span class="n">x_t</span><span class="p">,</span> <span class="n">edge_index</span><span class="o">=</span><span class="n">edge_index</span><span class="p">)</span>
</pre></div>
</div>
<p>For a correct mini-batching procedure in bipartite graphs, we need to tell <span class="inline-logo pyg">PyG</span> that it should increment source and target nodes of edges in <code class="xref py py-obj docutils literal notranslate"><span class="pre">edge_index</span></code> independently:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">BipartiteData</span><span class="p">(</span><span class="n">Data</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__inc__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s1">&#39;edge_index&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="bp">self</span><span class="o">.</span><span class="n">x_s</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">x_t</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)]])</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__inc__</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<p>Here, <code class="xref py py-obj docutils literal notranslate"><span class="pre">edge_index[0]</span></code> (the source nodes of edges) get incremented by <code class="xref py py-obj docutils literal notranslate"><span class="pre">x_s.size(0)</span></code> while <code class="xref py py-obj docutils literal notranslate"><span class="pre">edge_index[1]</span></code> (the target nodes of edges) get incremented by <code class="xref py py-obj docutils literal notranslate"><span class="pre">x_t.size(0)</span></code>.
We can again test our implementation by running a simple test script:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch_geometric.loader</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">x_s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>  <span class="c1"># 2 nodes.</span>
<span class="n">x_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>  <span class="c1"># 3 nodes.</span>
<span class="n">edge_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">BipartiteData</span><span class="p">(</span><span class="n">x_s</span><span class="o">=</span><span class="n">x_s</span><span class="p">,</span> <span class="n">x_t</span><span class="o">=</span><span class="n">x_t</span><span class="p">,</span> <span class="n">edge_index</span><span class="o">=</span><span class="n">edge_index</span><span class="p">)</span>

<span class="n">data_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="p">]</span>
<span class="n">loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">data_list</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">loader</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">BipartiteDataBatch</span><span class="p">(</span><span class="n">x_s</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span> <span class="n">x_t</span><span class="o">=</span><span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span> <span class="n">edge_index</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">edge_index</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span>
</pre></div>
</div>
<p>Again, this is exactly the behavior we aimed for!</p>
</section>
<section id="batching-along-new-dimensions">
<h2>Batching Along New Dimensions<a class="headerlink" href="#batching-along-new-dimensions" title="Permalink to this heading"></a></h2>
<p>Sometimes, attributes of <code class="xref py py-obj docutils literal notranslate"><span class="pre">data</span></code> objects should be batched by gaining a new batch dimension (as in classical mini-batching), <em>e.g.</em>, for graph-level properties or targets.
Specifically, a list of attributes of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">[num_features]</span></code> should be returned as <code class="xref py py-obj docutils literal notranslate"><span class="pre">[num_examples,</span> <span class="pre">num_features]</span></code> rather than <code class="xref py py-obj docutils literal notranslate"><span class="pre">[num_examples</span> <span class="pre">*</span> <span class="pre">num_features]</span></code>.
<span class="inline-logo pyg">PyG</span> achieves this by returning a concatenation dimension of <a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a> in <a class="reference internal" href="../generated/torch_geometric.data.Data.html#torch_geometric.data.Data.__cat_dim__" title="torch_geometric.data.Data.__cat_dim__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">__cat_dim__()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch_geometric.data</span> <span class="kn">import</span> <span class="n">Data</span>
<span class="kn">from</span> <span class="nn">torch_geometric.loader</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="k">class</span> <span class="nc">MyData</span><span class="p">(</span><span class="n">Data</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__cat_dim__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s1">&#39;foo&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__cat_dim__</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">edge_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
   <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
   <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="p">])</span>
<span class="n">foo</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">MyData</span><span class="p">(</span><span class="n">num_nodes</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">edge_index</span><span class="o">=</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">foo</span><span class="o">=</span><span class="n">foo</span><span class="p">)</span>

<span class="n">data_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="p">]</span>
<span class="n">loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">data_list</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">loader</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">MyDataBatch</span><span class="p">(</span><span class="n">num_nodes</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">edge_index</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="n">foo</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span>
</pre></div>
</div>
<p>As desired, <code class="xref py py-obj docutils literal notranslate"><span class="pre">batch.foo</span></code> is now described by two dimensions: The batch dimension and the feature dimension.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../tutorial/multi_node_multi_gpu_vanilla.html" class="btn btn-neutral float-left" title="Multi-Node Training using SLURM" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="sparse_tensor.html" class="btn btn-neutral float-right" title="Memory-Efficient Aggregations" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, PyG Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>